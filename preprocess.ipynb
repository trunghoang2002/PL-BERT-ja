{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import TimeoutError\n",
    "import os\n",
    "from pebble import ProcessPool\n",
    "import pickle\n",
    "\n",
    "from dataloader import build_dataloader as build_trainloader\n",
    "import datasets\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import pathlib\n",
    "# import phonemizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import BertJapaneseTokenizer\n",
    "import yaml\n",
    "\n",
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "from phonemize import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/.conda/envs/plbert/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##### config #####\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))\n",
    "\n",
    "##### set tokenizer #####\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(config['dataset_params']['tokenizer'])\n",
    "\n",
    "##### download dataset #####\n",
    "# comment out the following line in hogehoge/datasets/wikipedia/wikipedia.py\n",
    "# | \"Distribute\" >> beam.transforms.Reshuffle()\n",
    "datasets.config.DOWNLOADED_DATASETS_PATH = pathlib.Path(\"./dataset/wikipedia-ja\")\n",
    "dataset = datasets.load_dataset(\"wiki40b\", \"ja\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_START_ARTICLE_\\n弘前市立大和沢小学校\\n_START_SECTION_\\n学区\\n_START_PARAGRAPH_\\n大和沢、一野渡、狼森'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def decode_text(sample_text):\n",
    "    \"\"\"\n",
    "    Chuyển đổi chuỗi có dạng b'...' thành chuỗi UTF-8.\n",
    "    \n",
    "    Args:\n",
    "        sample_text (str): Chuỗi có thể chứa biểu diễn bytes dạng string.\n",
    "        \n",
    "    Returns:\n",
    "        str: Chuỗi đã giải mã UTF-8, hoặc giữ nguyên nếu không cần giải mã.\n",
    "    \"\"\"\n",
    "    if isinstance(sample_text, str):\n",
    "        if (sample_text.startswith(\"b'\") and sample_text.endswith(\"'\")) or (sample_text.startswith('b\"') and sample_text.endswith('\"')):\n",
    "            try:\n",
    "                sample_text = ast.literal_eval(sample_text)  # Chuyển từ string thành bytes thực sự\n",
    "                sample_text = sample_text.decode(\"utf-8\")   # Giải mã bytes thành UTF-8\n",
    "            except (SyntaxError, ValueError):\n",
    "                pass  # Trả về chuỗi gốc nếu gặp lỗi\n",
    "    return sample_text\n",
    "\n",
    "decoded_text = decode_text(dataset[3024]['text'])\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大和沢、一野渡、狼森'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_wiki_text(text):\n",
    "    text = ''.join(text.split('_START_PARAGRAPH_')[1:])\n",
    "    markers = [\"_START_ARTICLE_\", \"_START_SECTION_\", \"_START_PARAGRAPH_\", \"_NEWLINE_\", \"_START_HEADING_\", \n",
    "               \"_START_BULLET_\", \"_START_LIST_\", \"_START_TABLE_\", \"_START_CAPTION_\", \"_START_IMAGE_\"]\n",
    "    for marker in markers:\n",
    "        text = text.replace(marker, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "decoded_text = clean_wiki_text(decoded_text)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大和沢、一野渡、狼森'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = decode_text(text)\n",
    "    text = clean_wiki_text(text)\n",
    "    return text\n",
    "preprocess_text(dataset[3024]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  おはよう\n",
      "phonemes:  ['o', 'h', 'a', 'y', 'o', 'o']\n",
      "word:  ござい\n",
      "phonemes:  ['g', 'o', 'z', 'a', 'i']\n",
      "word:  ます\n",
      "phonemes:  ['m', 'a', 's', 'U']\n",
      "word:  。\n",
      "phonemes:  ['']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_make() in jcomon_label.c: No phoneme.\n"
     ]
    }
   ],
   "source": [
    "import pyopenjtalk\n",
    "import unicodedata\n",
    "from convert_label import openjtalk2julius\n",
    "\n",
    "_japanese = ['ky','sp', 'sh', 'ch', 'ts','ty', 'ry', 'ny', 'by', 'hy', 'gy', 'kw', 'gw', 'kj', 'gj', 'my', 'py','dy']\n",
    "japanese = ['$', '%', '&', '「', '」', '=', '~', '^', '|', '[', ']', '{', '}', '*', '+', '#', '<', '>']\n",
    "_japanese2japanese = {\n",
    "    'ky': '$',\n",
    "    'sp': '%',\n",
    "    'sh': '&',\n",
    "    'ch': '「',\n",
    "    'ts': '」',\n",
    "    'ty': '=',\n",
    "    'ry': '~',\n",
    "    'ny': '^',\n",
    "    'by': '|',\n",
    "    'hy': '[',\n",
    "    'gy': ']',\n",
    "    'kw': '{',\n",
    "    'gw': '}',\n",
    "    'kj': '*',\n",
    "    'gj': '+',\n",
    "    'my': '#',\n",
    "    'py': '<',\n",
    "    'dy': '>',\n",
    "}\n",
    "\n",
    "def global_phonemize(text: str):\n",
    "    phonemes = pyopenjtalk.g2p(text).split(' ')\n",
    "    print(\"phonemes: \", phonemes)\n",
    "    phonemes = [openjtalk2julius(p) for p in phonemes if p != '']\n",
    "    for i in range(len(phonemes)):\n",
    "        phoneme = phonemes[i]\n",
    "        if phoneme in _japanese:\n",
    "            phonemes[i] = _japanese2japanese[phoneme]\n",
    "    return phonemes\n",
    "\n",
    "text = unicodedata.normalize(\"NFKC\", \"おはようございます。\")\n",
    "words = tokenizer.tokenize(text)\n",
    "input_ids_ = tokenizer.convert_tokens_to_ids(words)\n",
    "phonemes = []\n",
    "input_ids = []\n",
    "for i in range(len(words)):\n",
    "    word = words[i]\n",
    "    input_id = input_ids_[i]\n",
    "    print(\"word: \", word)\n",
    "    phoneme = global_phonemize(word.replace('#', ''))\n",
    "    if len(phoneme) != 0:\n",
    "        phonemes.append(''.join(phoneme))\n",
    "        input_ids.append(input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: JPCommonLabel_make() in jcomon_label.c: No phoneme.\n",
      "WARNING: JPCommonLabel_make() in jcomon_label.c: No phoneme.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4612, 29331, 52, 28737, 29173, 10082, 29356],\n",
       " 'phonemes': ['yamato', 'sawa', 'i「i', 'no', 'watari', 'ookami', 'mori']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemize(preprocess_text(dataset[3024]['text']), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "num_shards = 1000\n",
    "def process_shard(i):\n",
    "    directory = root_directory+\"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    try:\n",
    "        shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "        processed_dataset = shard.map(lambda t: phonemize(preprocess_text(t['text']), tokenizer), remove_columns=['text'])\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        processed_dataset.save_to_disk(directory)\n",
    "        print(f'Shard {i} processed successfully.')\n",
    "        del processed_dataset  # Free memory\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f'Error processing shard {i}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số core của CPU là: 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Lấy số core của CPU\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Số core của CPU là: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = num_cores # change this to the number of CPU cores your machine has \n",
    "\n",
    "with ProcessPool(max_workers=max_workers) as pool:\n",
    "    pool.map(process_shard, range(num_shards), timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
